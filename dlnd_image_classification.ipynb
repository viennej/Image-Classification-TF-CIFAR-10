{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 21:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 3 Name: cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPQA0JugBoDFBDwCN/V8609somFO5qAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd5cde2470>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 21\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # Applied a method that we saw in class\n",
    "    min=np.min(x)\n",
    "    max=np.max(x)\n",
    "    return (x - min) / (max - min)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # Use function numpy.eye that return a 2-D array with ones on the diagonal and zeros elsewhere.\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return  tf.placeholder(tf.float32, shape=(None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.int32, shape=(None, n_classes), name='y')\n",
    "   \n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None), name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    w = tf.Variable(tf.truncated_normal((*conv_ksize, x_tensor.shape[3].value, conv_num_outputs), mean=0, stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    res = tf.nn.conv2d(x_tensor, w, strides=[1, *conv_strides, 1], padding='SAME')\n",
    "    res = tf.nn.relu(tf.nn.bias_add(res, b))\n",
    "    res = tf.nn.max_pool(res, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    w = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs], mean=0, stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, w), b))\n",
    "   \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    w = tf.Variable(tf.truncated_normal([int(x_tensor.get_shape()[1]), num_outputs], mean=0, stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.add(tf.matmul(x_tensor, w), b)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    res = conv2d_maxpool(x, 32, (8, 8), (1, 1), (2, 2), (2, 2))\n",
    "    res = conv2d_maxpool(x, 16, (4, 4), (1, 1), (2, 2), (2, 2))\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    res=flatten(res)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    res = fully_conn(res, 256)\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    res = fully_conn(res, 128)\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    res = fully_conn(res, 64)\n",
    "    res = tf.nn.dropout(res, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    res = output(res, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return res\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch,y: label_batch,keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    acc = session.run(accuracy, feed_dict={\n",
    "        x: valid_features,\n",
    "        y: valid_labels,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    print('Loss: ', loss, 'Validation Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 256\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  2.29919 Validation Accuracy:  0.1166\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  2.29663 Validation Accuracy:  0.1506\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  2.26361 Validation Accuracy:  0.1458\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  2.19088 Validation Accuracy:  0.234\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  2.09588 Validation Accuracy:  0.2816\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  2.01558 Validation Accuracy:  0.3132\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  1.94221 Validation Accuracy:  0.3276\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  1.84763 Validation Accuracy:  0.3554\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  1.87168 Validation Accuracy:  0.3452\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  1.71065 Validation Accuracy:  0.3732\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  1.66049 Validation Accuracy:  0.384\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  1.61421 Validation Accuracy:  0.3926\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  1.53099 Validation Accuracy:  0.4084\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  1.41107 Validation Accuracy:  0.4264\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  1.36808 Validation Accuracy:  0.4364\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  1.30483 Validation Accuracy:  0.4504\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  1.28525 Validation Accuracy:  0.4394\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  1.20671 Validation Accuracy:  0.4572\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  1.14753 Validation Accuracy:  0.47\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  1.13916 Validation Accuracy:  0.4636\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  1.04609 Validation Accuracy:  0.4734\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  1.00483 Validation Accuracy:  0.483\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  0.958602 Validation Accuracy:  0.482\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  0.911122 Validation Accuracy:  0.4812\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  0.870586 Validation Accuracy:  0.4924\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  0.838548 Validation Accuracy:  0.4994\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  0.800224 Validation Accuracy:  0.5046\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  0.791725 Validation Accuracy:  0.4986\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  0.743479 Validation Accuracy:  0.4942\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  0.694381 Validation Accuracy:  0.5142\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  0.693862 Validation Accuracy:  0.508\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  0.652515 Validation Accuracy:  0.5128\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  0.65665 Validation Accuracy:  0.5112\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  0.628381 Validation Accuracy:  0.5208\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  0.594015 Validation Accuracy:  0.5226\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  0.593693 Validation Accuracy:  0.5096\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  0.577774 Validation Accuracy:  0.5202\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  0.554375 Validation Accuracy:  0.5182\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  0.525172 Validation Accuracy:  0.5264\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  0.537059 Validation Accuracy:  0.5176\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  0.5133 Validation Accuracy:  0.5318\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  0.475896 Validation Accuracy:  0.5326\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  0.476811 Validation Accuracy:  0.533\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  0.447921 Validation Accuracy:  0.529\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  0.429271 Validation Accuracy:  0.5344\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  0.40881 Validation Accuracy:  0.5402\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  0.398196 Validation Accuracy:  0.5402\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  0.395092 Validation Accuracy:  0.5348\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  0.378797 Validation Accuracy:  0.539\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  0.36014 Validation Accuracy:  0.5446\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:  0.342301 Validation Accuracy:  0.5332\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:  0.339536 Validation Accuracy:  0.5414\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:  0.335133 Validation Accuracy:  0.5448\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:  0.299615 Validation Accuracy:  0.543\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:  0.318174 Validation Accuracy:  0.5386\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:  0.314536 Validation Accuracy:  0.5426\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:  0.301315 Validation Accuracy:  0.5438\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:  0.285954 Validation Accuracy:  0.5542\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:  0.272028 Validation Accuracy:  0.5464\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:  0.265483 Validation Accuracy:  0.545\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:  0.260841 Validation Accuracy:  0.5378\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:  0.247076 Validation Accuracy:  0.5446\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:  0.265542 Validation Accuracy:  0.547\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:  0.254667 Validation Accuracy:  0.547\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:  0.240532 Validation Accuracy:  0.5538\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:  0.22686 Validation Accuracy:  0.551\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:  0.225855 Validation Accuracy:  0.553\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:  0.218818 Validation Accuracy:  0.5482\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:  0.214282 Validation Accuracy:  0.5558\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:  0.187253 Validation Accuracy:  0.5516\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:  0.210198 Validation Accuracy:  0.5534\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:  0.191135 Validation Accuracy:  0.5556\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:  0.195534 Validation Accuracy:  0.5482\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:  0.186589 Validation Accuracy:  0.5562\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:  0.193029 Validation Accuracy:  0.5506\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:  0.181496 Validation Accuracy:  0.5584\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:  0.180295 Validation Accuracy:  0.5568\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:  0.172597 Validation Accuracy:  0.5546\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:  0.165633 Validation Accuracy:  0.5624\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:  0.166447 Validation Accuracy:  0.5536\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:  0.157689 Validation Accuracy:  0.5638\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:  0.148089 Validation Accuracy:  0.5624\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:  0.153396 Validation Accuracy:  0.553\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:  0.161191 Validation Accuracy:  0.558\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:  0.14304 Validation Accuracy:  0.5558\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:  0.129501 Validation Accuracy:  0.564\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:  0.137875 Validation Accuracy:  0.5576\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:  0.131345 Validation Accuracy:  0.5596\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:  0.12691 Validation Accuracy:  0.5588\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:  0.12264 Validation Accuracy:  0.5538\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:  0.132049 Validation Accuracy:  0.5452\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:  0.119882 Validation Accuracy:  0.5632\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:  0.12326 Validation Accuracy:  0.551\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:  0.109594 Validation Accuracy:  0.5592\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:  0.108533 Validation Accuracy:  0.5582\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:  0.103844 Validation Accuracy:  0.557\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:  0.114179 Validation Accuracy:  0.5588\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:  0.106537 Validation Accuracy:  0.5486\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:  0.0984847 Validation Accuracy:  0.5544\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:  0.0924331 Validation Accuracy:  0.5612\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:  0.079716 Validation Accuracy:  0.5636\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:  0.0910675 Validation Accuracy:  0.5582\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:  0.0980981 Validation Accuracy:  0.5636\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:  0.0857468 Validation Accuracy:  0.5618\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:  0.0891082 Validation Accuracy:  0.5656\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:  0.0823049 Validation Accuracy:  0.5526\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:  0.0753491 Validation Accuracy:  0.5578\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:  0.0737228 Validation Accuracy:  0.5624\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:  0.0630338 Validation Accuracy:  0.5678\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:  0.0699337 Validation Accuracy:  0.5606\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:  0.0695494 Validation Accuracy:  0.5642\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:  0.0648965 Validation Accuracy:  0.571\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:  0.06428 Validation Accuracy:  0.5692\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:  0.0610561 Validation Accuracy:  0.568\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:  0.058444 Validation Accuracy:  0.5708\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:  0.0541084 Validation Accuracy:  0.567\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:  0.0587815 Validation Accuracy:  0.5628\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:  0.0458673 Validation Accuracy:  0.5662\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:  0.0432561 Validation Accuracy:  0.5682\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:  0.0422281 Validation Accuracy:  0.5728\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:  0.0478886 Validation Accuracy:  0.5558\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:  0.0394232 Validation Accuracy:  0.5732\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:  0.0413967 Validation Accuracy:  0.5674\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:  0.0407867 Validation Accuracy:  0.562\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:  0.0450815 Validation Accuracy:  0.5628\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:  0.0415125 Validation Accuracy:  0.5664\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:  0.037955 Validation Accuracy:  0.5696\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:  0.0418204 Validation Accuracy:  0.5748\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:  0.03705 Validation Accuracy:  0.5738\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:  0.0312769 Validation Accuracy:  0.57\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:  0.0325026 Validation Accuracy:  0.5718\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:  0.0377099 Validation Accuracy:  0.5634\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:  0.0425445 Validation Accuracy:  0.5612\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:  0.0315256 Validation Accuracy:  0.5708\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:  0.0346885 Validation Accuracy:  0.5594\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:  0.0342955 Validation Accuracy:  0.569\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:  0.0333524 Validation Accuracy:  0.5678\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:  0.0283693 Validation Accuracy:  0.5756\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:  0.0240447 Validation Accuracy:  0.574\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:  0.0296595 Validation Accuracy:  0.5716\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:  0.0257535 Validation Accuracy:  0.572\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:  0.0224874 Validation Accuracy:  0.575\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:  0.021733 Validation Accuracy:  0.5668\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:  0.0208119 Validation Accuracy:  0.568\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:  0.0220238 Validation Accuracy:  0.567\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:  0.0217408 Validation Accuracy:  0.5642\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:  0.0201549 Validation Accuracy:  0.5658\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:  0.01635 Validation Accuracy:  0.5636\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:  0.0178672 Validation Accuracy:  0.5664\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:  0.0176586 Validation Accuracy:  0.5712\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:  0.0161751 Validation Accuracy:  0.563\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:  0.0207749 Validation Accuracy:  0.5658\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:  0.0198955 Validation Accuracy:  0.5706\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:  0.0186001 Validation Accuracy:  0.5632\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:  0.017625 Validation Accuracy:  0.5748\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:  0.0148822 Validation Accuracy:  0.571\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:  0.0181139 Validation Accuracy:  0.5694\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:  0.0157618 Validation Accuracy:  0.5638\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:  0.0198319 Validation Accuracy:  0.5618\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:  0.0164696 Validation Accuracy:  0.5588\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:  0.0204911 Validation Accuracy:  0.5658\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:  0.0141175 Validation Accuracy:  0.5754\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:  0.0126061 Validation Accuracy:  0.5676\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:  0.0126237 Validation Accuracy:  0.574\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:  0.0165521 Validation Accuracy:  0.5662\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:  0.0129656 Validation Accuracy:  0.5682\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:  0.0137391 Validation Accuracy:  0.566\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:  0.0169325 Validation Accuracy:  0.5568\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:  0.0158854 Validation Accuracy:  0.5646\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:  0.0113955 Validation Accuracy:  0.5704\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:  0.0103361 Validation Accuracy:  0.5706\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:  0.0116666 Validation Accuracy:  0.5684\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:  0.0125735 Validation Accuracy:  0.5646\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:  0.00974297 Validation Accuracy:  0.5732\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:  0.0118229 Validation Accuracy:  0.563\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:  0.0120273 Validation Accuracy:  0.5682\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:  0.0104452 Validation Accuracy:  0.5618\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:  0.00962181 Validation Accuracy:  0.575\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:  0.0120915 Validation Accuracy:  0.5636\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:  0.0104585 Validation Accuracy:  0.574\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:  0.0105103 Validation Accuracy:  0.5676\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:  0.0100869 Validation Accuracy:  0.562\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:  0.00785712 Validation Accuracy:  0.5732\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:  0.00676357 Validation Accuracy:  0.571\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:  0.00809769 Validation Accuracy:  0.5634\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:  0.0080887 Validation Accuracy:  0.558\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:  0.00593348 Validation Accuracy:  0.5694\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:  0.0074601 Validation Accuracy:  0.567\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:  0.00642609 Validation Accuracy:  0.5702\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:  0.00927616 Validation Accuracy:  0.5662\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:  0.0114791 Validation Accuracy:  0.558\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:  0.00766514 Validation Accuracy:  0.567\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:  0.00798395 Validation Accuracy:  0.5666\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:  0.00595727 Validation Accuracy:  0.563\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:  0.00673826 Validation Accuracy:  0.5666\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:  0.0072153 Validation Accuracy:  0.5626\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:  0.00660101 Validation Accuracy:  0.568\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:  0.00764934 Validation Accuracy:  0.5644\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:  0.00476013 Validation Accuracy:  0.5664\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:  0.00478719 Validation Accuracy:  0.5678\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:  0.00540957 Validation Accuracy:  0.561\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:  0.00469953 Validation Accuracy:  0.568\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:  0.00424135 Validation Accuracy:  0.5718\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:  0.00510596 Validation Accuracy:  0.568\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:  0.00441832 Validation Accuracy:  0.5636\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:  0.00339753 Validation Accuracy:  0.567\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:  0.00378132 Validation Accuracy:  0.5684\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:  0.00339377 Validation Accuracy:  0.5668\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:  0.00255309 Validation Accuracy:  0.5694\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:  0.00245586 Validation Accuracy:  0.5698\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:  0.00259808 Validation Accuracy:  0.5654\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:  0.00366804 Validation Accuracy:  0.5634\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:  0.00363147 Validation Accuracy:  0.5692\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:  0.00423387 Validation Accuracy:  0.5626\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:  0.00434392 Validation Accuracy:  0.5618\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:  0.00649694 Validation Accuracy:  0.553\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:  0.00353547 Validation Accuracy:  0.5608\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:  0.00257949 Validation Accuracy:  0.5674\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:  0.00408812 Validation Accuracy:  0.566\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:  0.00390048 Validation Accuracy:  0.568\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:  0.00296627 Validation Accuracy:  0.57\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:  0.00393318 Validation Accuracy:  0.5636\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:  0.00339229 Validation Accuracy:  0.5698\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:  0.00331659 Validation Accuracy:  0.5688\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:  0.0034855 Validation Accuracy:  0.5662\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:  0.00348262 Validation Accuracy:  0.5662\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:  0.00359643 Validation Accuracy:  0.5682\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:  0.00334598 Validation Accuracy:  0.5644\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:  0.00271739 Validation Accuracy:  0.569\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:  0.00350678 Validation Accuracy:  0.564\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:  0.00317888 Validation Accuracy:  0.5656\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:  0.00380829 Validation Accuracy:  0.5638\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:  0.00217504 Validation Accuracy:  0.5692\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:  0.00465124 Validation Accuracy:  0.5586\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:  0.00218 Validation Accuracy:  0.5688\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:  0.00201129 Validation Accuracy:  0.5642\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:  0.00295746 Validation Accuracy:  0.5688\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:  0.00222272 Validation Accuracy:  0.5734\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:  0.00219714 Validation Accuracy:  0.5704\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:  0.00273741 Validation Accuracy:  0.5532\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:  0.00164923 Validation Accuracy:  0.5716\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:  0.00218154 Validation Accuracy:  0.5702\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:  0.00197406 Validation Accuracy:  0.5716\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:  0.00195705 Validation Accuracy:  0.5768\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:  0.00140013 Validation Accuracy:  0.5752\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:  0.00200207 Validation Accuracy:  0.5696\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:  0.00164751 Validation Accuracy:  0.5752\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:  0.00173459 Validation Accuracy:  0.5714\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:  0.00190012 Validation Accuracy:  0.5728\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:  0.00196565 Validation Accuracy:  0.5672\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:  0.00168437 Validation Accuracy:  0.5642\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:  0.00232289 Validation Accuracy:  0.5662\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:  0.00160253 Validation Accuracy:  0.5718\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:  0.00133793 Validation Accuracy:  0.5742\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:  0.00214846 Validation Accuracy:  0.5668\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:  0.00184585 Validation Accuracy:  0.571\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:  2.29347 Validation Accuracy:  0.1748\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:  2.23621 Validation Accuracy:  0.1876\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:  2.00805 Validation Accuracy:  0.1848\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:  2.02069 Validation Accuracy:  0.218\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:  1.94794 Validation Accuracy:  0.2684\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:  2.01689 Validation Accuracy:  0.2812\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:  1.91694 Validation Accuracy:  0.293\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:  1.616 Validation Accuracy:  0.3266\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:  1.72945 Validation Accuracy:  0.3368\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:  1.6685 Validation Accuracy:  0.358\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:  1.83909 Validation Accuracy:  0.3678\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:  1.74062 Validation Accuracy:  0.3842\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:  1.47726 Validation Accuracy:  0.3994\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:  1.57429 Validation Accuracy:  0.408\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:  1.60347 Validation Accuracy:  0.4048\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:  1.74044 Validation Accuracy:  0.4134\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:  1.62413 Validation Accuracy:  0.423\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:  1.37091 Validation Accuracy:  0.4346\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:  1.4804 Validation Accuracy:  0.4424\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:  1.45923 Validation Accuracy:  0.431\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:  1.51269 Validation Accuracy:  0.4522\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:  1.47657 Validation Accuracy:  0.4454\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:  1.26985 Validation Accuracy:  0.4716\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:  1.41818 Validation Accuracy:  0.485\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:  1.32369 Validation Accuracy:  0.4842\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:  1.43859 Validation Accuracy:  0.4832\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:  1.30115 Validation Accuracy:  0.4884\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:  1.19066 Validation Accuracy:  0.4928\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:  1.32677 Validation Accuracy:  0.5036\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:  1.26675 Validation Accuracy:  0.5052\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:  1.31274 Validation Accuracy:  0.5134\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:  1.21342 Validation Accuracy:  0.5138\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:  1.12368 Validation Accuracy:  0.515\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:  1.24187 Validation Accuracy:  0.5314\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:  1.17844 Validation Accuracy:  0.5288\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:  1.19775 Validation Accuracy:  0.5298\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:  1.13514 Validation Accuracy:  0.5406\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:  1.06657 Validation Accuracy:  0.5312\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:  1.15516 Validation Accuracy:  0.5386\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:  1.11992 Validation Accuracy:  0.5246\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:  1.10695 Validation Accuracy:  0.5532\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:  1.03871 Validation Accuracy:  0.545\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:  1.00857 Validation Accuracy:  0.5418\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:  1.08691 Validation Accuracy:  0.553\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:  1.0224 Validation Accuracy:  0.5552\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:  1.05408 Validation Accuracy:  0.5658\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:  1.03267 Validation Accuracy:  0.541\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:  0.897298 Validation Accuracy:  0.5616\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:  1.02235 Validation Accuracy:  0.5632\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:  0.984501 Validation Accuracy:  0.5666\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:  1.0278 Validation Accuracy:  0.563\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:  1.02738 Validation Accuracy:  0.5596\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:  0.839464 Validation Accuracy:  0.5702\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:  1.01288 Validation Accuracy:  0.5704\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:  0.915845 Validation Accuracy:  0.566\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:  0.95464 Validation Accuracy:  0.5638\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:  0.879425 Validation Accuracy:  0.5692\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:  0.789124 Validation Accuracy:  0.5802\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:  0.932579 Validation Accuracy:  0.5792\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:  0.886889 Validation Accuracy:  0.573\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:  0.95125 Validation Accuracy:  0.5798\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:  0.831684 Validation Accuracy:  0.5854\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:  0.75662 Validation Accuracy:  0.5828\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:  0.922639 Validation Accuracy:  0.5836\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:  0.827087 Validation Accuracy:  0.5822\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:  0.856555 Validation Accuracy:  0.5828\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:  0.838405 Validation Accuracy:  0.59\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:  0.714362 Validation Accuracy:  0.5892\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:  0.865969 Validation Accuracy:  0.5866\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:  0.774461 Validation Accuracy:  0.5878\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:  0.787566 Validation Accuracy:  0.5916\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:  0.782255 Validation Accuracy:  0.5898\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:  0.69328 Validation Accuracy:  0.6006\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:  0.826539 Validation Accuracy:  0.597\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:  0.75004 Validation Accuracy:  0.5956\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:  0.79089 Validation Accuracy:  0.6018\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:  0.74367 Validation Accuracy:  0.591\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:  0.652716 Validation Accuracy:  0.5992\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:  0.758931 Validation Accuracy:  0.5988\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:  0.672887 Validation Accuracy:  0.6076\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:  0.777883 Validation Accuracy:  0.5976\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:  0.694535 Validation Accuracy:  0.6042\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:  0.592755 Validation Accuracy:  0.6036\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:  0.704787 Validation Accuracy:  0.6072\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:  0.647311 Validation Accuracy:  0.6122\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:  0.72046 Validation Accuracy:  0.6094\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:  0.681785 Validation Accuracy:  0.6076\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:  0.578699 Validation Accuracy:  0.6024\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:  0.700663 Validation Accuracy:  0.6114\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:  0.63943 Validation Accuracy:  0.61\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:  0.693329 Validation Accuracy:  0.6116\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:  0.674787 Validation Accuracy:  0.607\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:  0.57056 Validation Accuracy:  0.6152\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:  0.668514 Validation Accuracy:  0.6122\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:  0.62166 Validation Accuracy:  0.6118\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:  0.644954 Validation Accuracy:  0.6106\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:  0.634311 Validation Accuracy:  0.6136\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:  0.51924 Validation Accuracy:  0.6176\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:  0.616412 Validation Accuracy:  0.6192\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:  0.594743 Validation Accuracy:  0.6196\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:  0.644662 Validation Accuracy:  0.6132\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:  0.594294 Validation Accuracy:  0.6206\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:  0.498866 Validation Accuracy:  0.6176\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:  0.595389 Validation Accuracy:  0.6222\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:  0.565447 Validation Accuracy:  0.6156\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:  0.659183 Validation Accuracy:  0.6274\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:  0.607921 Validation Accuracy:  0.6186\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:  0.494918 Validation Accuracy:  0.6146\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:  0.573558 Validation Accuracy:  0.6238\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:  0.558493 Validation Accuracy:  0.6218\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:  0.611003 Validation Accuracy:  0.6242\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:  0.565447 Validation Accuracy:  0.6138\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:  0.463892 Validation Accuracy:  0.6246\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:  0.546477 Validation Accuracy:  0.6296\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:  0.534461 Validation Accuracy:  0.6278\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:  0.57126 Validation Accuracy:  0.63\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:  0.546926 Validation Accuracy:  0.6204\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:  0.465953 Validation Accuracy:  0.6194\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:  0.529139 Validation Accuracy:  0.6256\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:  0.494719 Validation Accuracy:  0.6318\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:  0.562219 Validation Accuracy:  0.629\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:  0.519584 Validation Accuracy:  0.6332\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:  0.436108 Validation Accuracy:  0.6256\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:  0.50332 Validation Accuracy:  0.6294\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:  0.505084 Validation Accuracy:  0.6236\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:  0.560948 Validation Accuracy:  0.629\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:  0.500707 Validation Accuracy:  0.6334\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:  0.425206 Validation Accuracy:  0.6306\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:  0.50312 Validation Accuracy:  0.6298\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:  0.507553 Validation Accuracy:  0.6242\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:  0.520264 Validation Accuracy:  0.632\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:  0.485367 Validation Accuracy:  0.6288\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:  0.42467 Validation Accuracy:  0.6272\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:  0.514324 Validation Accuracy:  0.6176\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:  0.473839 Validation Accuracy:  0.622\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:  0.510583 Validation Accuracy:  0.6288\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:  0.449025 Validation Accuracy:  0.6386\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:  0.384341 Validation Accuracy:  0.6408\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:  0.456129 Validation Accuracy:  0.6402\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:  0.458454 Validation Accuracy:  0.6394\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:  0.507533 Validation Accuracy:  0.64\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:  0.469864 Validation Accuracy:  0.634\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:  0.389319 Validation Accuracy:  0.6368\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:  0.48734 Validation Accuracy:  0.6274\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:  0.451652 Validation Accuracy:  0.637\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:  0.488365 Validation Accuracy:  0.6324\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:  0.446296 Validation Accuracy:  0.636\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:  0.394952 Validation Accuracy:  0.6386\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:  0.435987 Validation Accuracy:  0.6404\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:  0.421248 Validation Accuracy:  0.6334\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:  0.448804 Validation Accuracy:  0.634\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:  0.426361 Validation Accuracy:  0.6374\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:  0.385148 Validation Accuracy:  0.6416\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:  0.464964 Validation Accuracy:  0.647\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:  0.412265 Validation Accuracy:  0.6432\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:  0.459833 Validation Accuracy:  0.6376\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:  0.443957 Validation Accuracy:  0.6306\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:  0.380987 Validation Accuracy:  0.6286\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:  0.399539 Validation Accuracy:  0.6452\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:  0.414434 Validation Accuracy:  0.6348\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:  0.435349 Validation Accuracy:  0.6452\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:  0.41674 Validation Accuracy:  0.6434\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:  0.373821 Validation Accuracy:  0.6416\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:  0.40186 Validation Accuracy:  0.6424\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:  0.402169 Validation Accuracy:  0.6472\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:  0.422107 Validation Accuracy:  0.6386\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:  0.409491 Validation Accuracy:  0.6444\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:  0.343061 Validation Accuracy:  0.6448\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:  0.379491 Validation Accuracy:  0.6504\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:  0.369555 Validation Accuracy:  0.6454\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:  0.438056 Validation Accuracy:  0.6366\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:  0.389504 Validation Accuracy:  0.6434\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:  0.360847 Validation Accuracy:  0.6428\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:  0.386187 Validation Accuracy:  0.647\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:  0.381585 Validation Accuracy:  0.632\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:  0.41614 Validation Accuracy:  0.6456\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:  0.391131 Validation Accuracy:  0.64\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:  0.340851 Validation Accuracy:  0.6418\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:  0.382922 Validation Accuracy:  0.6466\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:  0.372494 Validation Accuracy:  0.6494\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:  0.407659 Validation Accuracy:  0.6528\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:  0.364396 Validation Accuracy:  0.651\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:  0.33175 Validation Accuracy:  0.6492\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:  0.366743 Validation Accuracy:  0.661\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:  0.347512 Validation Accuracy:  0.6518\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:  0.390077 Validation Accuracy:  0.6532\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:  0.367414 Validation Accuracy:  0.6416\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:  0.345958 Validation Accuracy:  0.6376\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:  0.364129 Validation Accuracy:  0.6458\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:  0.366754 Validation Accuracy:  0.6426\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:  0.392393 Validation Accuracy:  0.6502\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:  0.373366 Validation Accuracy:  0.6522\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:  0.312637 Validation Accuracy:  0.6546\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:  0.341155 Validation Accuracy:  0.6522\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:  0.337047 Validation Accuracy:  0.6352\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:  0.371192 Validation Accuracy:  0.6526\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:  0.3468 Validation Accuracy:  0.6546\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:  0.322523 Validation Accuracy:  0.6532\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:  0.348225 Validation Accuracy:  0.6514\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:  0.332274 Validation Accuracy:  0.6482\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:  0.369975 Validation Accuracy:  0.6448\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:  0.349062 Validation Accuracy:  0.6502\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:  0.303721 Validation Accuracy:  0.6484\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:  0.3508 Validation Accuracy:  0.6604\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:  0.299899 Validation Accuracy:  0.655\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:  0.352891 Validation Accuracy:  0.6594\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:  0.342975 Validation Accuracy:  0.662\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:  0.305095 Validation Accuracy:  0.65\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:  0.344124 Validation Accuracy:  0.6516\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:  0.321321 Validation Accuracy:  0.6454\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:  0.349599 Validation Accuracy:  0.647\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:  0.333536 Validation Accuracy:  0.6488\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:  0.303583 Validation Accuracy:  0.6528\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:  0.321308 Validation Accuracy:  0.6544\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:  0.297392 Validation Accuracy:  0.6578\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:  0.336999 Validation Accuracy:  0.655\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:  0.315783 Validation Accuracy:  0.6528\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:  0.282775 Validation Accuracy:  0.656\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:  0.312741 Validation Accuracy:  0.658\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:  0.288563 Validation Accuracy:  0.6558\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:  0.332801 Validation Accuracy:  0.6542\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:  0.329464 Validation Accuracy:  0.657\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:  0.309152 Validation Accuracy:  0.6546\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:  0.30092 Validation Accuracy:  0.6582\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:  0.29401 Validation Accuracy:  0.653\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:  0.331193 Validation Accuracy:  0.6476\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:  0.311197 Validation Accuracy:  0.6586\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:  0.274014 Validation Accuracy:  0.6566\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:  0.304339 Validation Accuracy:  0.6566\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:  0.282905 Validation Accuracy:  0.6484\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:  0.323334 Validation Accuracy:  0.656\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:  0.299807 Validation Accuracy:  0.6548\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:  0.274089 Validation Accuracy:  0.6628\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:  0.286186 Validation Accuracy:  0.6648\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:  0.263016 Validation Accuracy:  0.6554\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:  0.314788 Validation Accuracy:  0.6638\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:  0.298896 Validation Accuracy:  0.6616\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:  0.264851 Validation Accuracy:  0.6576\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:  0.269765 Validation Accuracy:  0.6618\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:  0.253457 Validation Accuracy:  0.6508\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:  0.317739 Validation Accuracy:  0.6546\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:  0.311895 Validation Accuracy:  0.6556\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:  0.276785 Validation Accuracy:  0.6542\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:  0.266762 Validation Accuracy:  0.6578\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:  0.257134 Validation Accuracy:  0.65\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:  0.288822 Validation Accuracy:  0.6608\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:  0.291694 Validation Accuracy:  0.6524\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:  0.2735 Validation Accuracy:  0.6594\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:  0.251589 Validation Accuracy:  0.6666\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:  0.258429 Validation Accuracy:  0.6572\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:  0.29738 Validation Accuracy:  0.664\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:  0.282243 Validation Accuracy:  0.6538\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:  0.273397 Validation Accuracy:  0.6614\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:  0.248332 Validation Accuracy:  0.662\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:  0.246443 Validation Accuracy:  0.6478\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:  0.292686 Validation Accuracy:  0.6552\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:  0.260388 Validation Accuracy:  0.6504\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:  0.262201 Validation Accuracy:  0.6602\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:  0.253634 Validation Accuracy:  0.6654\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:  0.244077 Validation Accuracy:  0.657\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:  0.294193 Validation Accuracy:  0.6594\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:  0.270898 Validation Accuracy:  0.66\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:  0.275084 Validation Accuracy:  0.6548\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:  0.242212 Validation Accuracy:  0.6664\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:  0.229748 Validation Accuracy:  0.6584\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:  0.289536 Validation Accuracy:  0.6592\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:  0.257163 Validation Accuracy:  0.6604\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:  0.254962 Validation Accuracy:  0.66\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:  0.263151 Validation Accuracy:  0.6594\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:  0.214141 Validation Accuracy:  0.6576\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:  0.289423 Validation Accuracy:  0.6594\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:  0.26978 Validation Accuracy:  0.667\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:  0.299245 Validation Accuracy:  0.6576\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:  0.249687 Validation Accuracy:  0.6676\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:  0.224026 Validation Accuracy:  0.6642\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:  0.28892 Validation Accuracy:  0.6564\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:  0.254653 Validation Accuracy:  0.6648\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:  0.25301 Validation Accuracy:  0.6652\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:  0.250553 Validation Accuracy:  0.6622\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:  0.221153 Validation Accuracy:  0.6582\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:  0.287117 Validation Accuracy:  0.6492\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:  0.271677 Validation Accuracy:  0.661\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:  0.267942 Validation Accuracy:  0.6616\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:  0.257407 Validation Accuracy:  0.6664\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:  0.208345 Validation Accuracy:  0.6662\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:  0.254976 Validation Accuracy:  0.6634\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:  0.24826 Validation Accuracy:  0.6628\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:  0.247304 Validation Accuracy:  0.6606\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:  0.230876 Validation Accuracy:  0.6562\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:  0.198736 Validation Accuracy:  0.6634\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:  0.26037 Validation Accuracy:  0.6624\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:  0.255777 Validation Accuracy:  0.6624\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:  0.259574 Validation Accuracy:  0.662\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:  0.238222 Validation Accuracy:  0.6604\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:  0.196357 Validation Accuracy:  0.667\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:  0.250843 Validation Accuracy:  0.6648\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:  0.253375 Validation Accuracy:  0.6558\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:  0.248223 Validation Accuracy:  0.6592\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:  0.224216 Validation Accuracy:  0.6682\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:  0.197445 Validation Accuracy:  0.6672\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:  0.239042 Validation Accuracy:  0.6546\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:  0.24313 Validation Accuracy:  0.662\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:  0.238522 Validation Accuracy:  0.667\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:  0.216505 Validation Accuracy:  0.6704\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:  0.193833 Validation Accuracy:  0.6676\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:  0.2429 Validation Accuracy:  0.6652\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:  0.226076 Validation Accuracy:  0.675\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:  0.232359 Validation Accuracy:  0.671\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:  0.210388 Validation Accuracy:  0.6738\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:  0.191625 Validation Accuracy:  0.668\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:  0.247043 Validation Accuracy:  0.66\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:  0.237567 Validation Accuracy:  0.6644\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:  0.234838 Validation Accuracy:  0.6696\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:  0.226082 Validation Accuracy:  0.6634\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:  0.187294 Validation Accuracy:  0.6702\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:  0.236998 Validation Accuracy:  0.668\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:  0.212561 Validation Accuracy:  0.6682\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:  0.228589 Validation Accuracy:  0.6612\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:  0.212688 Validation Accuracy:  0.6772\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:  0.198557 Validation Accuracy:  0.6626\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:  0.235256 Validation Accuracy:  0.6612\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:  0.208275 Validation Accuracy:  0.6704\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:  0.217371 Validation Accuracy:  0.6646\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:  0.199673 Validation Accuracy:  0.6776\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:  0.182288 Validation Accuracy:  0.659\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:  0.238064 Validation Accuracy:  0.6576\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:  0.216225 Validation Accuracy:  0.664\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:  0.22255 Validation Accuracy:  0.6654\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:  0.210568 Validation Accuracy:  0.6696\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:  0.184388 Validation Accuracy:  0.6686\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:  0.224516 Validation Accuracy:  0.6656\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:  0.212114 Validation Accuracy:  0.6626\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:  0.217292 Validation Accuracy:  0.667\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:  0.199741 Validation Accuracy:  0.6768\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:  0.167441 Validation Accuracy:  0.6746\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:  0.22393 Validation Accuracy:  0.6632\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:  0.224073 Validation Accuracy:  0.6716\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:  0.210329 Validation Accuracy:  0.6738\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:  0.200886 Validation Accuracy:  0.6828\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:  0.1638 Validation Accuracy:  0.6766\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:  0.213249 Validation Accuracy:  0.6798\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:  0.193222 Validation Accuracy:  0.6766\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:  0.205479 Validation Accuracy:  0.6684\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:  0.216793 Validation Accuracy:  0.6762\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:  0.161553 Validation Accuracy:  0.6766\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:  0.219854 Validation Accuracy:  0.6664\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:  0.192395 Validation Accuracy:  0.671\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:  0.196413 Validation Accuracy:  0.6738\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:  0.20105 Validation Accuracy:  0.6682\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:  0.177835 Validation Accuracy:  0.6708\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:  0.211635 Validation Accuracy:  0.6688\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:  0.194912 Validation Accuracy:  0.6704\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:  0.214645 Validation Accuracy:  0.662\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:  0.203384 Validation Accuracy:  0.6786\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:  0.16872 Validation Accuracy:  0.6736\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:  0.218035 Validation Accuracy:  0.6678\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:  0.204701 Validation Accuracy:  0.6678\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:  0.206619 Validation Accuracy:  0.6628\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:  0.200607 Validation Accuracy:  0.6746\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:  0.14522 Validation Accuracy:  0.6746\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:  0.194773 Validation Accuracy:  0.6784\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:  0.198316 Validation Accuracy:  0.677\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:  0.19226 Validation Accuracy:  0.6676\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:  0.190695 Validation Accuracy:  0.6718\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:  0.15903 Validation Accuracy:  0.6716\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:  0.214039 Validation Accuracy:  0.6684\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:  0.182239 Validation Accuracy:  0.6674\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:  0.202872 Validation Accuracy:  0.6708\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:  0.193159 Validation Accuracy:  0.6764\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:  0.153152 Validation Accuracy:  0.679\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:  0.193039 Validation Accuracy:  0.6776\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:  0.208704 Validation Accuracy:  0.6646\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:  0.199791 Validation Accuracy:  0.6704\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:  0.192192 Validation Accuracy:  0.6668\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:  0.151623 Validation Accuracy:  0.6692\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:  0.216156 Validation Accuracy:  0.6642\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:  0.213236 Validation Accuracy:  0.6684\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:  0.186215 Validation Accuracy:  0.6712\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:  0.182594 Validation Accuracy:  0.6834\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:  0.148481 Validation Accuracy:  0.6694\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:  0.187998 Validation Accuracy:  0.677\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:  0.200288 Validation Accuracy:  0.6666\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:  0.185113 Validation Accuracy:  0.668\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:  0.188058 Validation Accuracy:  0.6796\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:  0.138586 Validation Accuracy:  0.6744\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:  0.191268 Validation Accuracy:  0.6724\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:  0.171271 Validation Accuracy:  0.6754\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:  0.190554 Validation Accuracy:  0.6712\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:  0.17859 Validation Accuracy:  0.6776\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:  0.138243 Validation Accuracy:  0.6686\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:  0.190781 Validation Accuracy:  0.6706\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:  0.184237 Validation Accuracy:  0.6736\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:  0.1915 Validation Accuracy:  0.666\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:  0.20021 Validation Accuracy:  0.6768\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:  0.136647 Validation Accuracy:  0.6726\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:  0.185428 Validation Accuracy:  0.6752\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:  0.164698 Validation Accuracy:  0.6806\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:  0.188805 Validation Accuracy:  0.6748\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:  0.194246 Validation Accuracy:  0.6728\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:  0.12432 Validation Accuracy:  0.6816\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:  0.16762 Validation Accuracy:  0.668\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:  0.1727 Validation Accuracy:  0.6772\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:  0.182149 Validation Accuracy:  0.6724\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:  0.179948 Validation Accuracy:  0.6822\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:  0.124737 Validation Accuracy:  0.6862\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:  0.174561 Validation Accuracy:  0.6768\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:  0.171458 Validation Accuracy:  0.676\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:  0.178995 Validation Accuracy:  0.6658\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:  0.173756 Validation Accuracy:  0.675\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:  0.135697 Validation Accuracy:  0.6774\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:  0.169568 Validation Accuracy:  0.6682\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:  0.163129 Validation Accuracy:  0.6792\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:  0.177689 Validation Accuracy:  0.6796\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:  0.158131 Validation Accuracy:  0.6792\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:  0.119075 Validation Accuracy:  0.6828\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:  0.182563 Validation Accuracy:  0.6674\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:  0.174686 Validation Accuracy:  0.6726\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:  0.182209 Validation Accuracy:  0.6802\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:  0.164287 Validation Accuracy:  0.6838\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:  0.119094 Validation Accuracy:  0.6748\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:  0.16605 Validation Accuracy:  0.6802\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:  0.194833 Validation Accuracy:  0.6566\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:  0.194033 Validation Accuracy:  0.6698\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:  0.17129 Validation Accuracy:  0.6744\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:  0.119187 Validation Accuracy:  0.675\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:  0.186689 Validation Accuracy:  0.6642\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:  0.1619 Validation Accuracy:  0.6786\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:  0.174424 Validation Accuracy:  0.6704\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:  0.180588 Validation Accuracy:  0.6754\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:  0.123563 Validation Accuracy:  0.6748\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:  0.155283 Validation Accuracy:  0.6774\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:  0.16515 Validation Accuracy:  0.6788\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:  0.179008 Validation Accuracy:  0.6772\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:  0.170272 Validation Accuracy:  0.6828\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:  0.115677 Validation Accuracy:  0.6772\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:  0.165465 Validation Accuracy:  0.6728\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:  0.170993 Validation Accuracy:  0.677\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:  0.176878 Validation Accuracy:  0.6722\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:  0.160888 Validation Accuracy:  0.6876\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:  0.11988 Validation Accuracy:  0.6746\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:  0.152087 Validation Accuracy:  0.666\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:  0.165772 Validation Accuracy:  0.6722\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:  0.16864 Validation Accuracy:  0.6744\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:  0.172904 Validation Accuracy:  0.6742\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:  0.112921 Validation Accuracy:  0.6826\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:  0.14776 Validation Accuracy:  0.6788\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:  0.159776 Validation Accuracy:  0.6838\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:  0.165915 Validation Accuracy:  0.6754\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:  0.161768 Validation Accuracy:  0.6852\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:  0.113773 Validation Accuracy:  0.685\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:  0.168172 Validation Accuracy:  0.6788\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:  0.144272 Validation Accuracy:  0.6832\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:  0.158026 Validation Accuracy:  0.6794\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:  0.179281 Validation Accuracy:  0.6662\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:  0.111406 Validation Accuracy:  0.6838\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:  0.159058 Validation Accuracy:  0.6812\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:  0.156688 Validation Accuracy:  0.6698\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:  0.154069 Validation Accuracy:  0.6752\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:  0.15525 Validation Accuracy:  0.6846\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:  0.109855 Validation Accuracy:  0.6764\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:  0.133652 Validation Accuracy:  0.6898\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:  0.150869 Validation Accuracy:  0.687\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:  0.136993 Validation Accuracy:  0.678\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:  0.146473 Validation Accuracy:  0.6788\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:  0.100627 Validation Accuracy:  0.682\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:  0.129503 Validation Accuracy:  0.6808\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:  0.144539 Validation Accuracy:  0.675\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:  0.144727 Validation Accuracy:  0.6836\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:  0.14395 Validation Accuracy:  0.6754\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:  0.103085 Validation Accuracy:  0.681\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:  0.124604 Validation Accuracy:  0.6754\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:  0.136441 Validation Accuracy:  0.6754\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:  0.140823 Validation Accuracy:  0.6834\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:  0.158406 Validation Accuracy:  0.6714\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:  0.0951281 Validation Accuracy:  0.6872\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:  0.12835 Validation Accuracy:  0.6844\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:  0.148058 Validation Accuracy:  0.6724\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:  0.136849 Validation Accuracy:  0.6802\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:  0.151541 Validation Accuracy:  0.6776\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:  0.106073 Validation Accuracy:  0.6776\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:  0.125097 Validation Accuracy:  0.6742\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:  0.144004 Validation Accuracy:  0.6816\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:  0.144345 Validation Accuracy:  0.6716\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:  0.142923 Validation Accuracy:  0.6656\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:  0.0987554 Validation Accuracy:  0.6738\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:  0.133054 Validation Accuracy:  0.6692\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:  0.140033 Validation Accuracy:  0.6826\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:  0.138845 Validation Accuracy:  0.674\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:  0.141964 Validation Accuracy:  0.6802\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:  0.0941659 Validation Accuracy:  0.679\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:  0.123944 Validation Accuracy:  0.673\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:  0.125241 Validation Accuracy:  0.6754\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:  0.141467 Validation Accuracy:  0.665\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:  0.145852 Validation Accuracy:  0.6832\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:  0.0979609 Validation Accuracy:  0.673\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:  0.138398 Validation Accuracy:  0.675\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:  0.124129 Validation Accuracy:  0.6808\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:  0.134661 Validation Accuracy:  0.681\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:  0.151923 Validation Accuracy:  0.6786\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:  0.0899425 Validation Accuracy:  0.689\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:  0.120183 Validation Accuracy:  0.6738\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:  0.124384 Validation Accuracy:  0.6776\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:  0.13186 Validation Accuracy:  0.684\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:  0.146599 Validation Accuracy:  0.6768\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:  0.0951683 Validation Accuracy:  0.6682\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:  0.119701 Validation Accuracy:  0.6778\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:  0.118734 Validation Accuracy:  0.6816\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:  0.137091 Validation Accuracy:  0.678\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:  0.135687 Validation Accuracy:  0.6808\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:  0.090556 Validation Accuracy:  0.6808\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:  0.127976 Validation Accuracy:  0.679\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:  0.125663 Validation Accuracy:  0.68\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:  0.12823 Validation Accuracy:  0.681\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:  0.142422 Validation Accuracy:  0.6752\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:  0.0915652 Validation Accuracy:  0.6776\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:  0.127648 Validation Accuracy:  0.68\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:  0.116819 Validation Accuracy:  0.68\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:  0.131439 Validation Accuracy:  0.6848\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:  0.135537 Validation Accuracy:  0.6826\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:  0.0889228 Validation Accuracy:  0.6838\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:  0.11699 Validation Accuracy:  0.6772\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:  0.135365 Validation Accuracy:  0.6802\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:  0.131455 Validation Accuracy:  0.6804\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:  0.143094 Validation Accuracy:  0.6686\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:  0.0912417 Validation Accuracy:  0.6808\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:  0.11722 Validation Accuracy:  0.6778\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:  0.144978 Validation Accuracy:  0.657\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:  0.128507 Validation Accuracy:  0.6818\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:  0.13331 Validation Accuracy:  0.674\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:  0.0932677 Validation Accuracy:  0.6826\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:  0.12505 Validation Accuracy:  0.678\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:  0.121035 Validation Accuracy:  0.6812\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:  0.140868 Validation Accuracy:  0.6772\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:  0.127081 Validation Accuracy:  0.6768\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:  0.0873479 Validation Accuracy:  0.6754\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:  0.111636 Validation Accuracy:  0.68\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:  0.110912 Validation Accuracy:  0.6806\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:  0.128005 Validation Accuracy:  0.6734\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:  0.144068 Validation Accuracy:  0.6708\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:  0.0894701 Validation Accuracy:  0.6808\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:  0.118175 Validation Accuracy:  0.6756\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:  0.113395 Validation Accuracy:  0.6806\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:  0.114501 Validation Accuracy:  0.6774\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:  0.125194 Validation Accuracy:  0.6824\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:  0.0895807 Validation Accuracy:  0.6856\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:  0.102642 Validation Accuracy:  0.6838\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:  0.121007 Validation Accuracy:  0.6648\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:  0.12814 Validation Accuracy:  0.6822\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:  0.120913 Validation Accuracy:  0.6846\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:  0.0839838 Validation Accuracy:  0.6788\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:  0.103932 Validation Accuracy:  0.6794\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:  0.115476 Validation Accuracy:  0.6804\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:  0.122136 Validation Accuracy:  0.6866\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:  0.13325 Validation Accuracy:  0.6818\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:  0.0853568 Validation Accuracy:  0.6822\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:  0.119967 Validation Accuracy:  0.681\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:  0.122354 Validation Accuracy:  0.6856\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:  0.122337 Validation Accuracy:  0.6842\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:  0.121153 Validation Accuracy:  0.6796\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:  0.0820665 Validation Accuracy:  0.6802\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:  0.0980336 Validation Accuracy:  0.6776\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:  0.1107 Validation Accuracy:  0.6822\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:  0.118095 Validation Accuracy:  0.6846\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:  0.121984 Validation Accuracy:  0.6792\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:  0.0870939 Validation Accuracy:  0.6838\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:  0.107354 Validation Accuracy:  0.6834\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:  0.109332 Validation Accuracy:  0.6784\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:  0.120536 Validation Accuracy:  0.6786\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:  0.119092 Validation Accuracy:  0.6828\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:  0.0846724 Validation Accuracy:  0.6792\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:  0.108045 Validation Accuracy:  0.675\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:  0.110086 Validation Accuracy:  0.6814\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:  0.113976 Validation Accuracy:  0.6852\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:  0.115591 Validation Accuracy:  0.6854\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:  0.0749218 Validation Accuracy:  0.6802\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:  0.101282 Validation Accuracy:  0.6794\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:  0.116897 Validation Accuracy:  0.6814\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:  0.118831 Validation Accuracy:  0.6818\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:  0.116588 Validation Accuracy:  0.6864\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:  0.0831234 Validation Accuracy:  0.6798\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:  0.0955438 Validation Accuracy:  0.6844\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:  0.102035 Validation Accuracy:  0.6802\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:  0.0985714 Validation Accuracy:  0.6824\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:  0.119416 Validation Accuracy:  0.6832\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:  0.0912399 Validation Accuracy:  0.6752\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:  0.103974 Validation Accuracy:  0.682\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:  0.110916 Validation Accuracy:  0.6822\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:  0.090843 Validation Accuracy:  0.6824\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:  0.117092 Validation Accuracy:  0.685\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:  0.0810368 Validation Accuracy:  0.6842\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:  0.0943178 Validation Accuracy:  0.6776\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:  0.109313 Validation Accuracy:  0.679\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:  0.104176 Validation Accuracy:  0.6748\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:  0.110416 Validation Accuracy:  0.678\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:  0.0736248 Validation Accuracy:  0.673\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:  0.0920948 Validation Accuracy:  0.6734\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:  0.108349 Validation Accuracy:  0.6734\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:  0.105099 Validation Accuracy:  0.674\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:  0.107274 Validation Accuracy:  0.6806\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:  0.0765128 Validation Accuracy:  0.6796\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:  0.0902748 Validation Accuracy:  0.6758\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:  0.119841 Validation Accuracy:  0.6796\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:  0.109206 Validation Accuracy:  0.6764\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:  0.111936 Validation Accuracy:  0.683\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:  0.0766553 Validation Accuracy:  0.6796\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:  0.0820001 Validation Accuracy:  0.6756\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:  0.116571 Validation Accuracy:  0.6752\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:  0.116253 Validation Accuracy:  0.6758\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:  0.102578 Validation Accuracy:  0.6806\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:  0.068962 Validation Accuracy:  0.681\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:  0.0871267 Validation Accuracy:  0.6846\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:  0.102203 Validation Accuracy:  0.6826\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:  0.0901678 Validation Accuracy:  0.6844\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:  0.113064 Validation Accuracy:  0.6724\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:  0.0731476 Validation Accuracy:  0.6868\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:  0.0883205 Validation Accuracy:  0.683\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:  0.108592 Validation Accuracy:  0.6876\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:  0.102586 Validation Accuracy:  0.6888\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:  0.109764 Validation Accuracy:  0.6854\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:  0.0692355 Validation Accuracy:  0.6824\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:  0.0944039 Validation Accuracy:  0.674\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:  0.0995057 Validation Accuracy:  0.6844\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:  0.0957565 Validation Accuracy:  0.6826\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:  0.102755 Validation Accuracy:  0.6876\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:  0.079973 Validation Accuracy:  0.6848\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:  0.0908288 Validation Accuracy:  0.677\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:  0.10473 Validation Accuracy:  0.6792\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:  0.103055 Validation Accuracy:  0.6824\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:  0.100325 Validation Accuracy:  0.6866\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:  0.066734 Validation Accuracy:  0.6832\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:  0.0838501 Validation Accuracy:  0.6768\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:  0.0913129 Validation Accuracy:  0.6782\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:  0.104371 Validation Accuracy:  0.68\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:  0.10308 Validation Accuracy:  0.6822\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:  0.0763443 Validation Accuracy:  0.6756\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:  0.0780314 Validation Accuracy:  0.6768\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:  0.0865461 Validation Accuracy:  0.68\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:  0.102481 Validation Accuracy:  0.687\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:  0.0953319 Validation Accuracy:  0.6864\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:  0.0725687 Validation Accuracy:  0.6786\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:  0.0746796 Validation Accuracy:  0.6826\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:  0.0850652 Validation Accuracy:  0.6812\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:  0.10748 Validation Accuracy:  0.6798\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:  0.103401 Validation Accuracy:  0.6816\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:  0.0687482 Validation Accuracy:  0.6774\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:  0.0784436 Validation Accuracy:  0.6788\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:  0.0954668 Validation Accuracy:  0.6868\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:  0.103156 Validation Accuracy:  0.691\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:  0.107054 Validation Accuracy:  0.6854\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:  0.063471 Validation Accuracy:  0.6874\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:  0.0879826 Validation Accuracy:  0.6822\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:  0.0906293 Validation Accuracy:  0.6778\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:  0.105706 Validation Accuracy:  0.6812\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:  0.0952677 Validation Accuracy:  0.6812\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:  0.0687688 Validation Accuracy:  0.685\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:  0.0900581 Validation Accuracy:  0.6778\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:  0.0946322 Validation Accuracy:  0.6824\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:  0.102528 Validation Accuracy:  0.6786\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:  0.0901853 Validation Accuracy:  0.6826\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:  0.0648099 Validation Accuracy:  0.6872\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:  0.0846102 Validation Accuracy:  0.6826\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:  0.0957731 Validation Accuracy:  0.6802\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:  0.100529 Validation Accuracy:  0.6886\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:  0.104629 Validation Accuracy:  0.6816\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:  0.0655232 Validation Accuracy:  0.68\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:  0.0894134 Validation Accuracy:  0.6786\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:  0.0894769 Validation Accuracy:  0.6838\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:  0.0863183 Validation Accuracy:  0.685\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:  0.0856455 Validation Accuracy:  0.6876\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:  0.06894 Validation Accuracy:  0.6836\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:  0.0851886 Validation Accuracy:  0.6786\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:  0.0906521 Validation Accuracy:  0.6668\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:  0.0811639 Validation Accuracy:  0.6846\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:  0.0824269 Validation Accuracy:  0.6798\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:  0.0665712 Validation Accuracy:  0.6866\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:  0.0875729 Validation Accuracy:  0.6846\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:  0.0772479 Validation Accuracy:  0.6824\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:  0.0883982 Validation Accuracy:  0.679\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:  0.0895943 Validation Accuracy:  0.681\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:  0.0665454 Validation Accuracy:  0.6824\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:  0.0815299 Validation Accuracy:  0.6882\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:  0.083786 Validation Accuracy:  0.6822\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:  0.0771483 Validation Accuracy:  0.6828\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:  0.080521 Validation Accuracy:  0.6832\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:  0.0684567 Validation Accuracy:  0.686\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:  0.0717184 Validation Accuracy:  0.689\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:  0.0709311 Validation Accuracy:  0.6862\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:  0.0764904 Validation Accuracy:  0.6894\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:  0.0810446 Validation Accuracy:  0.689\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:  0.0551685 Validation Accuracy:  0.6872\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:  0.0819449 Validation Accuracy:  0.687\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:  0.0896827 Validation Accuracy:  0.6842\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:  0.091122 Validation Accuracy:  0.684\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:  0.0829141 Validation Accuracy:  0.6858\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:  0.0643194 Validation Accuracy:  0.6826\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:  0.0767082 Validation Accuracy:  0.6866\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:  0.0816482 Validation Accuracy:  0.6888\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:  0.0873853 Validation Accuracy:  0.685\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:  0.0827846 Validation Accuracy:  0.6828\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:  0.0702222 Validation Accuracy:  0.6814\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:  0.0736402 Validation Accuracy:  0.681\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:  0.072467 Validation Accuracy:  0.6886\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:  0.0918503 Validation Accuracy:  0.6834\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:  0.0926156 Validation Accuracy:  0.682\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:  0.0626494 Validation Accuracy:  0.6828\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:  0.0765075 Validation Accuracy:  0.6822\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:  0.0693852 Validation Accuracy:  0.6928\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:  0.0838034 Validation Accuracy:  0.6738\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:  0.0865418 Validation Accuracy:  0.6822\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:  0.0628645 Validation Accuracy:  0.6758\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:  0.070513 Validation Accuracy:  0.6848\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:  0.0798974 Validation Accuracy:  0.6822\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:  0.0888518 Validation Accuracy:  0.6832\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:  0.0949602 Validation Accuracy:  0.6784\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:  0.0651676 Validation Accuracy:  0.6824\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:  0.0771047 Validation Accuracy:  0.6758\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:  0.0831859 Validation Accuracy:  0.6722\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:  0.0870866 Validation Accuracy:  0.684\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:  0.0786057 Validation Accuracy:  0.6836\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:  0.0526982 Validation Accuracy:  0.6836\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:  0.0626988 Validation Accuracy:  0.681\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:  0.0819747 Validation Accuracy:  0.679\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:  0.078876 Validation Accuracy:  0.681\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:  0.0826067 Validation Accuracy:  0.6844\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:  0.0630068 Validation Accuracy:  0.6846\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:  0.0678254 Validation Accuracy:  0.6858\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:  0.0827851 Validation Accuracy:  0.6786\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:  0.0828575 Validation Accuracy:  0.6822\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:  0.0861779 Validation Accuracy:  0.6886\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:  0.0544813 Validation Accuracy:  0.6884\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:  0.065453 Validation Accuracy:  0.6776\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:  0.0755756 Validation Accuracy:  0.688\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:  0.0833995 Validation Accuracy:  0.6876\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:  0.0829358 Validation Accuracy:  0.689\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:  0.0547767 Validation Accuracy:  0.683\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:  0.0666561 Validation Accuracy:  0.6864\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:  0.07481 Validation Accuracy:  0.6832\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:  0.0794057 Validation Accuracy:  0.6866\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:  0.0877632 Validation Accuracy:  0.6902\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:  0.0580822 Validation Accuracy:  0.6828\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:  0.0589956 Validation Accuracy:  0.6804\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:  0.0654304 Validation Accuracy:  0.692\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:  0.0706516 Validation Accuracy:  0.6916\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:  0.08259 Validation Accuracy:  0.6822\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:  0.0556285 Validation Accuracy:  0.6846\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:  0.0702736 Validation Accuracy:  0.6804\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:  0.0723254 Validation Accuracy:  0.6904\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:  0.0636388 Validation Accuracy:  0.6888\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:  0.103547 Validation Accuracy:  0.6818\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:  0.0608185 Validation Accuracy:  0.689\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:  0.0636162 Validation Accuracy:  0.6798\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:  0.0643588 Validation Accuracy:  0.6892\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:  0.0643373 Validation Accuracy:  0.6892\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:  0.0829737 Validation Accuracy:  0.688\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:  0.0569001 Validation Accuracy:  0.6904\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:  0.0718152 Validation Accuracy:  0.686\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:  0.0749979 Validation Accuracy:  0.6894\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:  0.0712914 Validation Accuracy:  0.6852\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:  0.0876495 Validation Accuracy:  0.6846\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:  0.0580962 Validation Accuracy:  0.6806\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:  0.068368 Validation Accuracy:  0.6842\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:  0.0688184 Validation Accuracy:  0.6826\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:  0.0668322 Validation Accuracy:  0.689\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:  0.0817755 Validation Accuracy:  0.6866\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:  0.0523317 Validation Accuracy:  0.6862\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:  0.0670768 Validation Accuracy:  0.6778\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:  0.0736014 Validation Accuracy:  0.687\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:  0.0767926 Validation Accuracy:  0.6884\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:  0.0819057 Validation Accuracy:  0.6826\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:  0.0521211 Validation Accuracy:  0.6876\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:  0.0615599 Validation Accuracy:  0.6868\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:  0.0836086 Validation Accuracy:  0.6858\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:  0.0738366 Validation Accuracy:  0.6882\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:  0.0675266 Validation Accuracy:  0.69\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:  0.0482772 Validation Accuracy:  0.681\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:  0.0722724 Validation Accuracy:  0.683\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:  0.0637062 Validation Accuracy:  0.686\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:  0.0782202 Validation Accuracy:  0.6844\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:  0.0791342 Validation Accuracy:  0.6856\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:  0.0557618 Validation Accuracy:  0.6818\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:  0.0593615 Validation Accuracy:  0.6882\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:  0.0702005 Validation Accuracy:  0.6826\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:  0.0726311 Validation Accuracy:  0.6878\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:  0.0773859 Validation Accuracy:  0.683\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:  0.0497035 Validation Accuracy:  0.686\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:  0.0719397 Validation Accuracy:  0.6766\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:  0.0659051 Validation Accuracy:  0.6848\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:  0.0694361 Validation Accuracy:  0.6882\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:  0.0771474 Validation Accuracy:  0.6886\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:  0.049563 Validation Accuracy:  0.6868\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:  0.0579558 Validation Accuracy:  0.6834\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:  0.070155 Validation Accuracy:  0.6922\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:  0.063003 Validation Accuracy:  0.6916\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:  0.077674 Validation Accuracy:  0.6806\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:  0.0550252 Validation Accuracy:  0.6912\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:  0.0633213 Validation Accuracy:  0.6816\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:  0.0716294 Validation Accuracy:  0.682\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:  0.0598259 Validation Accuracy:  0.6892\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:  0.0847148 Validation Accuracy:  0.6828\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:  0.0510903 Validation Accuracy:  0.6826\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:  0.0624413 Validation Accuracy:  0.6832\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:  0.062632 Validation Accuracy:  0.6884\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:  0.0634599 Validation Accuracy:  0.689\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:  0.0906022 Validation Accuracy:  0.6868\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:  0.0496741 Validation Accuracy:  0.6876\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:  0.0633956 Validation Accuracy:  0.6848\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:  0.060528 Validation Accuracy:  0.6922\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:  0.0664025 Validation Accuracy:  0.6904\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:  0.0853417 Validation Accuracy:  0.6814\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:  0.0487787 Validation Accuracy:  0.6872\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:  0.0560858 Validation Accuracy:  0.6832\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:  0.0629059 Validation Accuracy:  0.69\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:  0.0688546 Validation Accuracy:  0.694\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:  0.0891683 Validation Accuracy:  0.6816\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:  0.0525374 Validation Accuracy:  0.6854\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:  0.0599729 Validation Accuracy:  0.686\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:  0.0625978 Validation Accuracy:  0.6884\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:  0.0610711 Validation Accuracy:  0.6922\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:  0.0760133 Validation Accuracy:  0.6856\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:  0.0452641 Validation Accuracy:  0.6916\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:  0.0580179 Validation Accuracy:  0.6804\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:  0.0570631 Validation Accuracy:  0.6852\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:  0.0632567 Validation Accuracy:  0.6856\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:  0.0765985 Validation Accuracy:  0.6852\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:  0.0534747 Validation Accuracy:  0.6756\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:  0.0610709 Validation Accuracy:  0.6862\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:  0.0698318 Validation Accuracy:  0.6856\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:  0.0684604 Validation Accuracy:  0.69\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:  0.0829657 Validation Accuracy:  0.6882\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:  0.0482688 Validation Accuracy:  0.6848\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:  0.0524399 Validation Accuracy:  0.6794\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:  0.0568893 Validation Accuracy:  0.69\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:  0.0721797 Validation Accuracy:  0.6848\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:  0.0876365 Validation Accuracy:  0.688\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:  0.0591881 Validation Accuracy:  0.6846\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:  0.0546928 Validation Accuracy:  0.6818\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:  0.0636627 Validation Accuracy:  0.6828\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:  0.0633529 Validation Accuracy:  0.6922\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:  0.0838274 Validation Accuracy:  0.6858\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:  0.0450427 Validation Accuracy:  0.6868\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:  0.0684121 Validation Accuracy:  0.6854\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:  0.0590946 Validation Accuracy:  0.6908\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:  0.0627044 Validation Accuracy:  0.68\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:  0.0761492 Validation Accuracy:  0.689\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:  0.047521 Validation Accuracy:  0.6814\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:  0.0527563 Validation Accuracy:  0.677\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:  0.0553107 Validation Accuracy:  0.6886\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:  0.0690399 Validation Accuracy:  0.6882\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:  0.0817491 Validation Accuracy:  0.691\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:  0.0396495 Validation Accuracy:  0.6894\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:  0.0634089 Validation Accuracy:  0.6838\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:  0.0527983 Validation Accuracy:  0.683\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:  0.0559065 Validation Accuracy:  0.6912\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:  0.0776005 Validation Accuracy:  0.6862\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:  0.0472365 Validation Accuracy:  0.6896\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:  0.0550922 Validation Accuracy:  0.6862\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:  0.0563041 Validation Accuracy:  0.6886\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:  0.0678437 Validation Accuracy:  0.685\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:  0.0763023 Validation Accuracy:  0.6874\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:  0.0430065 Validation Accuracy:  0.6886\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:  0.0518176 Validation Accuracy:  0.6898\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:  0.0567251 Validation Accuracy:  0.682\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:  0.0674223 Validation Accuracy:  0.6866\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:  0.0714304 Validation Accuracy:  0.6842\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:  0.0468087 Validation Accuracy:  0.692\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:  0.0492003 Validation Accuracy:  0.6872\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:  0.0640352 Validation Accuracy:  0.688\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:  0.0586746 Validation Accuracy:  0.6936\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:  0.0736259 Validation Accuracy:  0.6872\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:  0.0544919 Validation Accuracy:  0.6786\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:  0.0719758 Validation Accuracy:  0.6696\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:  0.0604546 Validation Accuracy:  0.6898\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:  0.058291 Validation Accuracy:  0.6882\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:  0.0725767 Validation Accuracy:  0.6876\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:  0.0353406 Validation Accuracy:  0.6898\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:  0.053101 Validation Accuracy:  0.6824\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:  0.0597768 Validation Accuracy:  0.689\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:  0.058001 Validation Accuracy:  0.689\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:  0.0752635 Validation Accuracy:  0.684\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:  0.0471825 Validation Accuracy:  0.682\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:  0.0498849 Validation Accuracy:  0.6768\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:  0.0589431 Validation Accuracy:  0.68\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:  0.0586627 Validation Accuracy:  0.6826\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:  0.071159 Validation Accuracy:  0.6852\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:  0.0468463 Validation Accuracy:  0.6834\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:  0.0577601 Validation Accuracy:  0.683\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:  0.0503596 Validation Accuracy:  0.6862\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:  0.0542124 Validation Accuracy:  0.6882\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:  0.0734216 Validation Accuracy:  0.6858\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:  0.0426001 Validation Accuracy:  0.6904\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:  0.0482574 Validation Accuracy:  0.6778\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:  0.0595987 Validation Accuracy:  0.6858\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:  0.056761 Validation Accuracy:  0.6928\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:  0.0634914 Validation Accuracy:  0.6922\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:  0.0464904 Validation Accuracy:  0.6806\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:  0.0539896 Validation Accuracy:  0.6864\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:  0.0526851 Validation Accuracy:  0.6904\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:  0.0552688 Validation Accuracy:  0.697\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:  0.0683707 Validation Accuracy:  0.6856\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:  0.0407655 Validation Accuracy:  0.6904\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:  0.0412825 Validation Accuracy:  0.6846\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:  0.0488712 Validation Accuracy:  0.6832\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:  0.0601077 Validation Accuracy:  0.6862\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:  0.0681185 Validation Accuracy:  0.6866\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:  0.041221 Validation Accuracy:  0.687\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:  0.0457995 Validation Accuracy:  0.6812\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:  0.0501511 Validation Accuracy:  0.6918\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:  0.0629876 Validation Accuracy:  0.6848\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:  0.0605327 Validation Accuracy:  0.6896\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:  0.0329001 Validation Accuracy:  0.6844\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:  0.0472521 Validation Accuracy:  0.6832\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:  0.0491151 Validation Accuracy:  0.686\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:  0.0552393 Validation Accuracy:  0.688\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:  0.0725392 Validation Accuracy:  0.6832\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:  0.0366883 Validation Accuracy:  0.6884\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:  0.0559192 Validation Accuracy:  0.6782\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:  0.0502305 Validation Accuracy:  0.6884\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:  0.0537862 Validation Accuracy:  0.6828\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:  0.0636496 Validation Accuracy:  0.686\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:  0.0417057 Validation Accuracy:  0.6802\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:  0.0479787 Validation Accuracy:  0.681\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:  0.0479269 Validation Accuracy:  0.6904\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:  0.0538404 Validation Accuracy:  0.6834\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:  0.0692192 Validation Accuracy:  0.6798\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:  0.0439847 Validation Accuracy:  0.6838\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:  0.0478841 Validation Accuracy:  0.686\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:  0.0500295 Validation Accuracy:  0.6858\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:  0.0492128 Validation Accuracy:  0.6896\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:  0.0673392 Validation Accuracy:  0.686\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:  0.0460698 Validation Accuracy:  0.6836\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:  0.0469856 Validation Accuracy:  0.6804\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:  0.0412288 Validation Accuracy:  0.683\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:  0.0506927 Validation Accuracy:  0.6804\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:  0.0664482 Validation Accuracy:  0.6846\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:  0.0395283 Validation Accuracy:  0.68\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:  0.0493839 Validation Accuracy:  0.6744\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:  0.0443752 Validation Accuracy:  0.6898\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:  0.0423611 Validation Accuracy:  0.6826\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:  0.0681855 Validation Accuracy:  0.6788\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:  0.0382614 Validation Accuracy:  0.6768\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:  0.0415118 Validation Accuracy:  0.6826\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:  0.0487495 Validation Accuracy:  0.6838\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:  0.0487585 Validation Accuracy:  0.685\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:  0.0645517 Validation Accuracy:  0.685\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:  0.040887 Validation Accuracy:  0.6806\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:  0.0452287 Validation Accuracy:  0.679\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:  0.0486097 Validation Accuracy:  0.6826\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:  0.048426 Validation Accuracy:  0.6818\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:  0.0633142 Validation Accuracy:  0.6856\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:  0.0393522 Validation Accuracy:  0.6828\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:  0.0497957 Validation Accuracy:  0.678\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:  0.0524075 Validation Accuracy:  0.6832\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:  0.0584548 Validation Accuracy:  0.6864\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:  0.061671 Validation Accuracy:  0.6852\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:  0.0430428 Validation Accuracy:  0.6804\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:  0.0398598 Validation Accuracy:  0.682\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:  0.0471391 Validation Accuracy:  0.6862\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:  0.0467883 Validation Accuracy:  0.6808\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:  0.0587715 Validation Accuracy:  0.684\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:  0.0416186 Validation Accuracy:  0.6828\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:  0.039638 Validation Accuracy:  0.6736\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:  0.0376318 Validation Accuracy:  0.6846\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:  0.0407995 Validation Accuracy:  0.684\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:  0.0546315 Validation Accuracy:  0.69\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:  0.0395837 Validation Accuracy:  0.6824\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:  0.0420575 Validation Accuracy:  0.6858\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:  0.0432954 Validation Accuracy:  0.6866\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:  0.0514101 Validation Accuracy:  0.681\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:  0.0600273 Validation Accuracy:  0.6924\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:  0.0385798 Validation Accuracy:  0.6828\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:  0.0408834 Validation Accuracy:  0.6824\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:  0.0389403 Validation Accuracy:  0.6838\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:  0.0523633 Validation Accuracy:  0.686\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:  0.0624153 Validation Accuracy:  0.6838\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:  0.0392243 Validation Accuracy:  0.682\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:  0.048075 Validation Accuracy:  0.6876\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:  0.0379537 Validation Accuracy:  0.6934\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:  0.0528464 Validation Accuracy:  0.69\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:  0.0654604 Validation Accuracy:  0.679\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:  0.0398623 Validation Accuracy:  0.6804\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:  0.0459808 Validation Accuracy:  0.6844\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:  0.0430876 Validation Accuracy:  0.6814\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:  0.0398901 Validation Accuracy:  0.6848\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:  0.060307 Validation Accuracy:  0.6842\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:  0.0358889 Validation Accuracy:  0.6868\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:  0.0375875 Validation Accuracy:  0.6826\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:  0.0408293 Validation Accuracy:  0.6886\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:  0.0457159 Validation Accuracy:  0.6822\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:  0.0628625 Validation Accuracy:  0.6904\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:  0.0409804 Validation Accuracy:  0.6852\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:  0.047025 Validation Accuracy:  0.6852\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:  0.0370811 Validation Accuracy:  0.6778\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:  0.048363 Validation Accuracy:  0.687\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:  0.0637194 Validation Accuracy:  0.6892\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:  0.0415612 Validation Accuracy:  0.681\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:  0.0462348 Validation Accuracy:  0.681\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:  0.0395479 Validation Accuracy:  0.6804\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:  0.044501 Validation Accuracy:  0.686\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:  0.0558802 Validation Accuracy:  0.691\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:  0.0386603 Validation Accuracy:  0.6834\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:  0.0429825 Validation Accuracy:  0.687\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss:  0.0377872 Validation Accuracy:  0.6896\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss:  0.0464316 Validation Accuracy:  0.689\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss:  0.0554721 Validation Accuracy:  0.6918\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss:  0.0353241 Validation Accuracy:  0.6788\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:  0.0353818 Validation Accuracy:  0.6838\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss:  0.0421905 Validation Accuracy:  0.683\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss:  0.0498011 Validation Accuracy:  0.6846\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss:  0.0645118 Validation Accuracy:  0.6914\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss:  0.0387345 Validation Accuracy:  0.6784\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:  0.0396847 Validation Accuracy:  0.6856\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss:  0.0424496 Validation Accuracy:  0.6894\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss:  0.0530003 Validation Accuracy:  0.6852\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss:  0.0590782 Validation Accuracy:  0.6944\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss:  0.041563 Validation Accuracy:  0.6858\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:  0.0486185 Validation Accuracy:  0.684\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss:  0.0407334 Validation Accuracy:  0.6904\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss:  0.0404112 Validation Accuracy:  0.6904\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss:  0.0581329 Validation Accuracy:  0.6908\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss:  0.0419395 Validation Accuracy:  0.6886\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:  0.0459607 Validation Accuracy:  0.6846\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss:  0.0411552 Validation Accuracy:  0.686\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss:  0.0319248 Validation Accuracy:  0.6968\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss:  0.0543744 Validation Accuracy:  0.6974\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss:  0.0402327 Validation Accuracy:  0.6802\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:  0.0420517 Validation Accuracy:  0.6844\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss:  0.0311379 Validation Accuracy:  0.687\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss:  0.0316622 Validation Accuracy:  0.6872\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss:  0.062834 Validation Accuracy:  0.6928\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss:  0.0328741 Validation Accuracy:  0.6912\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:  0.0336815 Validation Accuracy:  0.6888\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss:  0.0423769 Validation Accuracy:  0.6938\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss:  0.0384911 Validation Accuracy:  0.6944\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss:  0.0596574 Validation Accuracy:  0.69\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss:  0.0359912 Validation Accuracy:  0.6938\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:  0.0442668 Validation Accuracy:  0.6874\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss:  0.0422695 Validation Accuracy:  0.6884\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss:  0.0431153 Validation Accuracy:  0.6896\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss:  0.055171 Validation Accuracy:  0.6892\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss:  0.03658 Validation Accuracy:  0.6794\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:  0.0378743 Validation Accuracy:  0.6882\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss:  0.0352583 Validation Accuracy:  0.6878\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss:  0.0379293 Validation Accuracy:  0.6884\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss:  0.0619271 Validation Accuracy:  0.6944\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss:  0.0346243 Validation Accuracy:  0.6822\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:  0.0391903 Validation Accuracy:  0.6766\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss:  0.0414459 Validation Accuracy:  0.684\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss:  0.0379476 Validation Accuracy:  0.6866\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss:  0.0553913 Validation Accuracy:  0.6928\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss:  0.0361804 Validation Accuracy:  0.684\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:  0.0396813 Validation Accuracy:  0.6834\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss:  0.0346642 Validation Accuracy:  0.6882\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss:  0.0380265 Validation Accuracy:  0.6906\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss:  0.0594918 Validation Accuracy:  0.685\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss:  0.0399091 Validation Accuracy:  0.6882\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:  0.0382963 Validation Accuracy:  0.6888\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss:  0.0340232 Validation Accuracy:  0.6884\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss:  0.0479546 Validation Accuracy:  0.6878\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss:  0.0573713 Validation Accuracy:  0.6876\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss:  0.0327705 Validation Accuracy:  0.6882\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:  0.0385845 Validation Accuracy:  0.6802\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss:  0.0363736 Validation Accuracy:  0.6892\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss:  0.0438099 Validation Accuracy:  0.683\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss:  0.0495645 Validation Accuracy:  0.6906\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss:  0.039942 Validation Accuracy:  0.6918\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:  0.0388561 Validation Accuracy:  0.6796\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss:  0.0364941 Validation Accuracy:  0.6872\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss:  0.0347262 Validation Accuracy:  0.6866\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss:  0.0475104 Validation Accuracy:  0.6842\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss:  0.0314205 Validation Accuracy:  0.6898\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:  0.0372375 Validation Accuracy:  0.6828\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss:  0.033618 Validation Accuracy:  0.69\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss:  0.0382664 Validation Accuracy:  0.6842\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss:  0.0508104 Validation Accuracy:  0.6866\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss:  0.0409262 Validation Accuracy:  0.6894\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:  0.0355867 Validation Accuracy:  0.6784\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss:  0.0285396 Validation Accuracy:  0.6852\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss:  0.0398936 Validation Accuracy:  0.6872\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss:  0.0519314 Validation Accuracy:  0.6908\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss:  0.0371667 Validation Accuracy:  0.6866\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:  0.0343277 Validation Accuracy:  0.6872\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss:  0.03226 Validation Accuracy:  0.6936\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss:  0.0398781 Validation Accuracy:  0.6926\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss:  0.0518203 Validation Accuracy:  0.69\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss:  0.0308658 Validation Accuracy:  0.6806\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:  0.0368633 Validation Accuracy:  0.6884\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss:  0.0391024 Validation Accuracy:  0.6868\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss:  0.0427026 Validation Accuracy:  0.6872\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss:  0.0581429 Validation Accuracy:  0.6836\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss:  0.0315599 Validation Accuracy:  0.6876\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:  0.0284522 Validation Accuracy:  0.685\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss:  0.0296972 Validation Accuracy:  0.6898\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss:  0.0392058 Validation Accuracy:  0.6884\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss:  0.054555 Validation Accuracy:  0.6826\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss:  0.0361284 Validation Accuracy:  0.6828\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:  0.0351429 Validation Accuracy:  0.6846\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss:  0.0255293 Validation Accuracy:  0.6822\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss:  0.0415063 Validation Accuracy:  0.6858\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss:  0.0458036 Validation Accuracy:  0.6874\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss:  0.0301266 Validation Accuracy:  0.686\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:  0.030855 Validation Accuracy:  0.6854\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss:  0.0330916 Validation Accuracy:  0.687\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss:  0.0298734 Validation Accuracy:  0.688\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss:  0.0477431 Validation Accuracy:  0.6844\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss:  0.032642 Validation Accuracy:  0.683\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:  0.0343907 Validation Accuracy:  0.683\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss:  0.0323369 Validation Accuracy:  0.6908\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss:  0.0353299 Validation Accuracy:  0.689\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss:  0.0432747 Validation Accuracy:  0.684\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss:  0.0302519 Validation Accuracy:  0.6856\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:  0.0315525 Validation Accuracy:  0.682\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss:  0.0368288 Validation Accuracy:  0.6882\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss:  0.0370192 Validation Accuracy:  0.688\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss:  0.058737 Validation Accuracy:  0.6898\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss:  0.0303048 Validation Accuracy:  0.6902\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:  0.0287018 Validation Accuracy:  0.6884\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss:  0.028 Validation Accuracy:  0.691\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss:  0.0466166 Validation Accuracy:  0.68\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss:  0.0470506 Validation Accuracy:  0.6914\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss:  0.03147 Validation Accuracy:  0.6816\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:  0.0466707 Validation Accuracy:  0.6856\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss:  0.0362818 Validation Accuracy:  0.6872\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss:  0.0336178 Validation Accuracy:  0.6952\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss:  0.0430949 Validation Accuracy:  0.6888\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss:  0.023851 Validation Accuracy:  0.6856\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:  0.0324144 Validation Accuracy:  0.6884\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss:  0.0239385 Validation Accuracy:  0.6912\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss:  0.0377149 Validation Accuracy:  0.6864\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss:  0.045652 Validation Accuracy:  0.6822\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss:  0.0308797 Validation Accuracy:  0.684\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:  0.036799 Validation Accuracy:  0.6884\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss:  0.0323165 Validation Accuracy:  0.6918\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss:  0.0375016 Validation Accuracy:  0.688\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss:  0.0436681 Validation Accuracy:  0.6904\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss:  0.0268696 Validation Accuracy:  0.6874\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:  0.0400843 Validation Accuracy:  0.683\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss:  0.0340006 Validation Accuracy:  0.6912\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss:  0.0386622 Validation Accuracy:  0.6914\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss:  0.0489698 Validation Accuracy:  0.6892\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss:  0.0368977 Validation Accuracy:  0.6834\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:  0.0325853 Validation Accuracy:  0.6846\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss:  0.0307757 Validation Accuracy:  0.6884\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss:  0.0325823 Validation Accuracy:  0.6862\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss:  0.039657 Validation Accuracy:  0.6888\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss:  0.0343133 Validation Accuracy:  0.6862\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:  0.029524 Validation Accuracy:  0.6868\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss:  0.0297313 Validation Accuracy:  0.6946\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss:  0.0331891 Validation Accuracy:  0.692\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss:  0.0493395 Validation Accuracy:  0.6892\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss:  0.0289349 Validation Accuracy:  0.6914\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:  0.0322286 Validation Accuracy:  0.6868\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss:  0.036042 Validation Accuracy:  0.6848\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss:  0.0327943 Validation Accuracy:  0.6934\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss:  0.0466227 Validation Accuracy:  0.6878\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss:  0.0323686 Validation Accuracy:  0.6872\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:  0.0442696 Validation Accuracy:  0.6796\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss:  0.0277939 Validation Accuracy:  0.6904\n",
      "Epoch 232, CIFAR-10 Batch 3:  Loss:  0.0339195 Validation Accuracy:  0.6948\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss:  0.0474088 Validation Accuracy:  0.69\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss:  0.0315181 Validation Accuracy:  0.688\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:  0.0328212 Validation Accuracy:  0.68\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss:  0.0358457 Validation Accuracy:  0.689\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss:  0.0386566 Validation Accuracy:  0.691\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss:  0.0418946 Validation Accuracy:  0.6862\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss:  0.0214461 Validation Accuracy:  0.6934\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:  0.0281078 Validation Accuracy:  0.6894\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss:  0.027262 Validation Accuracy:  0.6892\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss:  0.0319927 Validation Accuracy:  0.6886\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss:  0.0430309 Validation Accuracy:  0.6906\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss:  0.0277749 Validation Accuracy:  0.6928\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:  0.0323416 Validation Accuracy:  0.691\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss:  0.0321175 Validation Accuracy:  0.6838\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss:  0.0384993 Validation Accuracy:  0.6926\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss:  0.0480713 Validation Accuracy:  0.6946\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss:  0.0245041 Validation Accuracy:  0.6918\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:  0.0398828 Validation Accuracy:  0.6874\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss:  0.0259198 Validation Accuracy:  0.692\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss:  0.0334311 Validation Accuracy:  0.6956\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss:  0.0419436 Validation Accuracy:  0.6972\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss:  0.0304755 Validation Accuracy:  0.6896\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:  0.0348319 Validation Accuracy:  0.6912\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss:  0.0310255 Validation Accuracy:  0.6964\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss:  0.0377536 Validation Accuracy:  0.69\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss:  0.0501059 Validation Accuracy:  0.6946\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss:  0.0253526 Validation Accuracy:  0.687\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:  0.0266317 Validation Accuracy:  0.6848\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss:  0.0307776 Validation Accuracy:  0.6932\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss:  0.0330385 Validation Accuracy:  0.6936\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss:  0.0392015 Validation Accuracy:  0.698\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss:  0.025404 Validation Accuracy:  0.6928\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:  0.032595 Validation Accuracy:  0.6948\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss:  0.0311511 Validation Accuracy:  0.6938\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss:  0.0391602 Validation Accuracy:  0.6958\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss:  0.0415932 Validation Accuracy:  0.6934\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss:  0.0246323 Validation Accuracy:  0.6958\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:  0.0346254 Validation Accuracy:  0.6888\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss:  0.0307845 Validation Accuracy:  0.6914\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss:  0.0306482 Validation Accuracy:  0.6926\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss:  0.0504821 Validation Accuracy:  0.6936\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss:  0.0266907 Validation Accuracy:  0.6854\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:  0.0324224 Validation Accuracy:  0.6858\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss:  0.0381307 Validation Accuracy:  0.6808\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss:  0.0366523 Validation Accuracy:  0.6926\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss:  0.0495578 Validation Accuracy:  0.695\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss:  0.0247163 Validation Accuracy:  0.6898\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:  0.035069 Validation Accuracy:  0.6904\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss:  0.0310361 Validation Accuracy:  0.6916\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss:  0.0322122 Validation Accuracy:  0.6936\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss:  0.0501194 Validation Accuracy:  0.6882\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss:  0.0258982 Validation Accuracy:  0.6888\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:  0.0385945 Validation Accuracy:  0.6894\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss:  0.034209 Validation Accuracy:  0.6888\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss:  0.0332357 Validation Accuracy:  0.6952\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss:  0.0582524 Validation Accuracy:  0.6898\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss:  0.026089 Validation Accuracy:  0.6962\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:  0.0304791 Validation Accuracy:  0.6888\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss:  0.0263927 Validation Accuracy:  0.694\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss:  0.0379441 Validation Accuracy:  0.6942\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss:  0.0503207 Validation Accuracy:  0.693\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss:  0.0231275 Validation Accuracy:  0.6924\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:  0.0310004 Validation Accuracy:  0.6854\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss:  0.029702 Validation Accuracy:  0.6934\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss:  0.0269059 Validation Accuracy:  0.6984\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss:  0.0484973 Validation Accuracy:  0.6964\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss:  0.0254917 Validation Accuracy:  0.6904\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:  0.0380729 Validation Accuracy:  0.6832\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss:  0.0327457 Validation Accuracy:  0.692\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss:  0.0251187 Validation Accuracy:  0.6964\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss:  0.0427235 Validation Accuracy:  0.6944\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss:  0.0242054 Validation Accuracy:  0.6928\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:  0.0328294 Validation Accuracy:  0.69\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss:  0.0282209 Validation Accuracy:  0.6908\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss:  0.0252383 Validation Accuracy:  0.692\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss:  0.0492054 Validation Accuracy:  0.692\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss:  0.0242023 Validation Accuracy:  0.6904\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:  0.0315565 Validation Accuracy:  0.6836\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss:  0.0379886 Validation Accuracy:  0.6856\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss:  0.034614 Validation Accuracy:  0.6922\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss:  0.0470342 Validation Accuracy:  0.6916\n",
      "Epoch 248, CIFAR-10 Batch 5:  Loss:  0.0234517 Validation Accuracy:  0.6898\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:  0.03385 Validation Accuracy:  0.694\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss:  0.0310296 Validation Accuracy:  0.6946\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss:  0.0369537 Validation Accuracy:  0.6878\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss:  0.0453527 Validation Accuracy:  0.6956\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss:  0.0212457 Validation Accuracy:  0.6912\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:  0.0269603 Validation Accuracy:  0.6908\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss:  0.0290346 Validation Accuracy:  0.6908\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss:  0.0298448 Validation Accuracy:  0.6948\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss:  0.0533115 Validation Accuracy:  0.69\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss:  0.025847 Validation Accuracy:  0.6846\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:  0.042477 Validation Accuracy:  0.6832\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss:  0.0323062 Validation Accuracy:  0.691\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss:  0.0258086 Validation Accuracy:  0.6964\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss:  0.0454102 Validation Accuracy:  0.6964\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss:  0.0236508 Validation Accuracy:  0.6986\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:  0.0300975 Validation Accuracy:  0.6944\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss:  0.0282007 Validation Accuracy:  0.6924\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss:  0.0280236 Validation Accuracy:  0.6952\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss:  0.0445331 Validation Accuracy:  0.6976\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss:  0.0261992 Validation Accuracy:  0.6884\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:  0.0345275 Validation Accuracy:  0.6932\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss:  0.0318125 Validation Accuracy:  0.696\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss:  0.0345057 Validation Accuracy:  0.6934\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss:  0.0454915 Validation Accuracy:  0.6824\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss:  0.0267932 Validation Accuracy:  0.6816\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:  0.040851 Validation Accuracy:  0.6818\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss:  0.0342266 Validation Accuracy:  0.6874\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss:  0.0309956 Validation Accuracy:  0.6946\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss:  0.0435853 Validation Accuracy:  0.6932\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss:  0.0211601 Validation Accuracy:  0.6872\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:  0.03256 Validation Accuracy:  0.6898\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss:  0.0328073 Validation Accuracy:  0.6904\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss:  0.0317963 Validation Accuracy:  0.6926\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss:  0.0470424 Validation Accuracy:  0.6908\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss:  0.0249521 Validation Accuracy:  0.6882\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:  0.0359081 Validation Accuracy:  0.6878\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss:  0.0315862 Validation Accuracy:  0.6908\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss:  0.0268374 Validation Accuracy:  0.6952\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss:  0.04326 Validation Accuracy:  0.6902\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss:  0.01931 Validation Accuracy:  0.688\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.68916015625\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fccd04692b0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
